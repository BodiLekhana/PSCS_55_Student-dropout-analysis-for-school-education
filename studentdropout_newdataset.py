# -*- coding: utf-8 -*-
"""STUDENTDROPOUT-NEWDATASET.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fyX_4_Dq8reu_W6WrZL_O-2Ljjc5YiO8
"""

import pandas as pd
import numpy as np
data=pd.read_csv("dataset.csv")
print(data.shape)
data.head(10)

print(data.describe())
data.info()

# Libraries for implementation and checking performance of ML models

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder          # converts string values to numeric values
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from sklearn import tree
import seaborn as sns
from sklearn.tree import plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

# Drop the 'Unnamed' columns if they exist in the dataframe
# Identify columns that start with 'Unnamed:'
unnamed_cols = [col for col in data.columns if 'Unnamed:' in col]
if unnamed_cols:
    data = data.drop(columns=unnamed_cols)

# Encode the Target column
label_encoder = LabelEncoder()
data['Target'] = label_encoder.fit_transform(data['Target'])

# Separate features (X) and target (y)
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)

RFmodel = RandomForestClassifier(n_estimators=100, random_state=42)
# estimators - number of decision trees in forest

RFmodel.fit(X_train, y_train)
# used to train machine learning models. It adjusts the parameters of the model based on the provided data.

# to generate predictions for new or unseen data
Y_pred = RFmodel.predict(X_test)

features = data.iloc[:, :-1].columns
class_names = label_encoder.classes_ # Get class names from the label_encoder
plt.figure(figsize=(20,10))  # Set figure size to make the tree more readable
# Plot the first decision tree from the Random Forest model
plot_tree(RFmodel.estimators_[0],
          feature_names=features,
          class_names=class_names,
          filled=True,
          rounded=True)
plt.title("One Decision Tree from the Random Forest")
plt.show()

print("random forest performance")

accuracy = accuracy_score(y_test, Y_pred)
print("Accuracy:", accuracy)
print("---------------------------------------------------")

cm = confusion_matrix(y_test, Y_pred)
print("Confusion Matrix:\n", cm)
print("---------------------------------------------------")

# Classification Report for precision, recall, F1-score
print("Classification Report:\n", classification_report(y_test, Y_pred))

from sklearn.preprocessing import LabelBinarizer

# Binarize the target variable for multiclass ROC
label_binarizer = LabelBinarizer()
y_test_binarized = label_binarizer.fit_transform(y_test)

# Get predicted probabilities for all classes
y_prob = RFmodel.predict_proba(X_test)

plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(len(label_binarizer.classes_)):
    class_name = label_binarizer.classes_[i]
    fpr, tpr, thresholds = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'ROC Curve for {class_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Random Forest\nAccuracy: {:.2f}%'.format(
    accuracy * 100))
plt.legend(loc="lower right")
plt.show()

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

print("knn performance")

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("---------------------------------------------------")

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)
print("---------------------------------------------------")

# Classification Report for precision, recall, F1-score
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.preprocessing import LabelBinarizer

# Binarize the target variable for multiclass ROC
label_binarizer = LabelBinarizer()
y_test_binarized = label_binarizer.fit_transform(y_test)

# Get predicted probabilities for all classes
y_prob = RFmodel.predict_proba(X_test)

plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(len(label_binarizer.classes_)):
    class_name = label_binarizer.classes_[i]
    fpr, tpr, thresholds = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'ROC Curve for {class_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for kNN\nAccuracy: {:.2f}%'.format(
    accuracy * 100))
plt.legend(loc="lower right")
plt.show()

LRmodel = LogisticRegression()
LRmodel.fit(X_train, y_train)

y_pred = LRmodel.predict(X_test)

print("linear regression performance")

accuracy = accuracy_score(y_test, Y_pred)
print("Accuracy:", accuracy)
print("---------------------------------------------------")

cm = confusion_matrix(y_test, Y_pred)
print("Confusion Matrix:\n", cm)
print("---------------------------------------------------")

# Classification Report for precision, recall, F1-score
print("Classification Report:\n", classification_report(y_test, Y_pred))

from sklearn.preprocessing import LabelBinarizer

# Binarize the target variable for multiclass ROC
label_binarizer = LabelBinarizer()
y_test_binarized = label_binarizer.fit_transform(y_test)

# Get predicted probabilities for all classes
y_prob = RFmodel.predict_proba(X_test)

plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(len(label_binarizer.classes_)):
    class_name = label_binarizer.classes_[i]
    fpr, tpr, thresholds = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'ROC Curve for {class_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Linear Regression\nAccuracy: {:.2f}%'.format(
    accuracy * 100))
plt.legend(loc="lower right")
plt.show()

clf_entropy = DecisionTreeClassifier(criterion="entropy", random_state=100, max_depth=3, min_samples_leaf=5)

clf_entropy.fit(X_train, y_train)

Y_pred = clf_entropy.predict(X_test)

print("decision tree performance using entropy")

accuracy = accuracy_score(y_test, Y_pred)
print("Accuracy:", accuracy)
print("---------------------------------------------------")

cm = confusion_matrix(y_test, Y_pred)
print("Confusion Matrix:\n", cm)
print("---------------------------------------------------")

# Classification Report for precision, recall, F1-score
print("Classification Report:\n", classification_report(y_test, Y_pred))

from sklearn.preprocessing import LabelBinarizer

# Binarize the target variable for multiclass ROC
label_binarizer = LabelBinarizer()
y_test_binarized = label_binarizer.fit_transform(y_test)

# Get predicted probabilities for all classes
y_prob = RFmodel.predict_proba(X_test)

plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(len(label_binarizer.classes_)):
    class_name = label_binarizer.classes_[i]
    fpr, tpr, thresholds = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'ROC Curve for {class_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Decision Tree\nAccuracy: {:.2f}%'.format(
    accuracy * 100))
plt.legend(loc="lower right")
plt.show()

import shap
explainer = shap.Explainer(clf_entropy)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test)

nb_classifier = GaussianNB()

nb_classifier.fit(X_train, y_train)

y_pred = nb_classifier.predict(X_test)

print("naive bayesian performance")

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("---------------------------------------------------")

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)
print("---------------------------------------------------")

# Classification Report for precision, recall, F1-score
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.preprocessing import LabelBinarizer

# Binarize the target variable for multiclass ROC
label_binarizer = LabelBinarizer()
y_test_binarized = label_binarizer.fit_transform(y_test)

# Get predicted probabilities for all classes
y_prob = RFmodel.predict_proba(X_test)

plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(len(label_binarizer.classes_)):
    class_name = label_binarizer.classes_[i]
    fpr, tpr, thresholds = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'ROC Curve for {class_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Naive Bayes\nAccuracy: {:.2f}%'.format(
    accuracy * 100))
plt.legend(loc="lower right")
plt.show()

svm = SVC(kernel='linear', C=1.0, random_state=42)
# kernel='linear': uses a linear kernel for classification
# C=1.0: regularization parameter to control margin vs misclassification
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)

print("SVM performance")

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("---------------------------------------------------")

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)
print("---------------------------------------------------")

# Classification Report for precision, recall, F1-score
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.preprocessing import LabelBinarizer

# Binarize the target variable for multiclass ROC
label_binarizer = LabelBinarizer()
y_test_binarized = label_binarizer.fit_transform(y_test)

# Get predicted probabilities for all classes
y_prob = RFmodel.predict_proba(X_test)

plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(len(label_binarizer.classes_)):
    class_name = label_binarizer.classes_[i]
    fpr, tpr, thresholds = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'ROC Curve for {class_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for SVM\nAccuracy: {:.2f}%'.format(
    accuracy * 100))
plt.legend(loc="lower right")
plt.show()

gb_clf = GradientBoostingClassifier(n_estimators=100,learning_rate=0.1, max_depth=3, random_state=42)
gb_clf.fit(X_train, y_train)
y_pred = gb_clf.predict(X_test)

print("gradient boosting performance")

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("---------------------------------------------------")

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)
print("---------------------------------------------------")

# Classification Report for precision, recall, F1-score
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.preprocessing import LabelBinarizer

# Binarize the target variable for multiclass ROC
label_binarizer = LabelBinarizer()
y_test_binarized = label_binarizer.fit_transform(y_test)

# Get predicted probabilities for all classes
y_prob = RFmodel.predict_proba(X_test)

plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(len(label_binarizer.classes_)):
    class_name = label_binarizer.classes_[i]
    fpr, tpr, thresholds = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'ROC Curve for {class_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Gradient Boosting\nAccuracy: {:.2f}%'.format(
    accuracy * 100))
plt.legend(loc="lower right")
plt.show()

df_new = pd.get_dummies(data)
corr_matrix = df_new.corr()

plt.figure(figsize=(32, 24))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

import shap
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder

data_for_shap = pd.read_csv("dataset.csv")

# X_shap will be a DataFrame to retain feature names for SHAP plots
X_shap = data_for_shap.drop('Target', axis=1)
y_shap = data_for_shap['Target']

# Encode the target variable
label_encoder = LabelEncoder()
y_shap_encoded = label_encoder.fit_transform(y_shap)

X_train_shap, X_test_shap, y_train_shap, y_test_shap = train_test_split(X_shap, y_shap_encoded, test_size=0.2, random_state=42)

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(kernel='linear', C=1.0, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth= 3, random_state=42),
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier(criterion="entropy", random_state=100, max_depth=3, min_samples_leaf=5),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

for name, model in models.items():
    model.fit(X_train_shap, y_train_shap)
    print(f"{name} trained successfully.")

# SHAP Analysis
for name, model in models.items():
    print(f"\nExplaining {name}...")

    if name in ["Gradient Boosting", "Naive Bayes", "KNN"]:
        print(f"Skipping SHAP explanation for {name} due to multi-class limitation or unsupported explainer type.")
        continue # Skip to the next model

    if name in ["Random Forest", "Decision Tree"]: # Using TreeExplainer for tree-based models that support multi-class
        explainer = shap.TreeExplainer(model)
        # For multi-class classification, shap_values can be a list of arrays
        shap_values = explainer.shap_values(X_test_shap)
    else:  # Using LinearExplainer for linear models or others where appropriate
        # LinearExplainer works well for models where feature effects are additive
        # For non-linear models, KernelExplainer might be more appropriate but is computationally intensive.
        explainer = shap.LinearExplainer(model, X_train_shap, feature_perturbation="interventional")
        shap_values = explainer.shap_values(X_test_shap)

    # SHAP Summary Plot
    plt.figure(figsize=(20, 16))
    shap.summary_plot(shap_values, X_test_shap, show=False)
    plt.title(f"SHAP Summary Plot for {name}")
    plt.tight_layout() # Adjust layout to prevent labels overlapping
    plt.show()

    # SHAP Bar Plot (average feature importance)
    plt.figure(figsize=(20, 16))
    shap.summary_plot(shap_values, X_test_shap, plot_type="bar", show=False)
    plt.title(f"Feature Importance (SHAP Bar Plot) - {name}")
    plt.tight_layout() # Adjust layout to prevent labels overlapping
    plt.show()

print("SHAP analysis completed for all models.")